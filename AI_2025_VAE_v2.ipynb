{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanogiagu/corso_AI_2025/blob/main/AI_2025_VAE_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelli generativi basati su VAE\n",
        "\n",
        "scopo: familiarizzare con la progettazione e l'uso di archietture generative basate su VAE\n",
        "DL tool: pytorch\n",
        "\n",
        "### tasks ###\n",
        "\n",
        "1.   implementazione di una beta-VAE (eg una VAE con un iperparametro che controlla il peso relativo del termine di divergenza rispetto al termine di ricostruzione nella loss per la generazione di immagini realistiche\n",
        "\n",
        "\n",
        "Referenze:\n",
        "\n",
        "*   [VAE](https://arxiv.org/abs/1312.6114) e [beta-VAE](https://openreview.net/forum?id=Sy2fzU9gl)\n",
        "\n",
        "\n",
        "\n",
        "**Datasets:**\n",
        "\n",
        "*  [fashion-MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST)\n",
        "\n"
      ],
      "metadata": {
        "id": "8ix-CMEfVfV2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxsEpzXeKlYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736ffba5-b9cf-4197-c8d4-95f03deaf818"
      },
      "source": [
        "# import delle librerie\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "il dataset FashionMNIST è costituito da immagini di icone di oggetti di viestiario corrispondenti a 60k immagini per il training e 10k immagini per il test.\n",
        "\n",
        "Ogni immagine ha dimensione $(28,28)$ pixel, con un canale di intensità (scala di grigi) a 8 bit (valori di intensità di ogni pixel $\\in [0,256]$).\n",
        "\n",
        "È disponibile in varie librerie per il ML (scikit-learn, torch, tensorflow, librerie esterne...). Prendendo il dataset già disponibile in pytorch possiamo sfruttare il vantaggio di di avere conversioni automatiche in tensori torch e normalizzazioni. Leggendo il dataset da un input esterno, come per esempio fatto nell'esercitazione su PCA prendendolo da openm, in formato di ndarray numpy, dovremo convertire noi i vettori in tensori torch e normalizzarli, ma risulterà più chiaro come usare in generale datset esterni. in questo hands-on utilizzeremo questo secondo approccio."
      ],
      "metadata": {
        "id": "EAGMqoujgjzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lettura del dataset Fashion-MNIST da openml usando la libreria sklearn\n",
        "# label:\n",
        "# 0 T-shirt/top\n",
        "# 1 Trouser\n",
        "# 2 Pullover\n",
        "# 3 Dress\n",
        "# 4 Coat\n",
        "# 5 Sandal\n",
        "# 6 Shirt\n",
        "# 7 Sneaker\n",
        "# 8 Bag\n",
        "# 9 Ankle boot\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "images, labels = fetch_openml(\"Fashion-MNIST\", version=1, return_X_y=True, as_frame=False, parser=\"pandas\")\n",
        "labels = labels.astype(int) # converte le label in int\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uigwnUjOW3i",
        "outputId": "0fef64fb-cb6b-4108-f1c5-45aa0be423ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grafichiamo 9 immagini random\n",
        "figure = plt.figure(figsize=(5, 4))\n",
        "cols, rows = 3,3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = np.random.randint(len(images))\n",
        "    img, label = images[sample_idx], labels[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.reshape((28,28)), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "PdNlVOCEguAw",
        "outputId": "8293ad7b-8b2e-4746-b7b2-61e70bf96251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting del campione in training e test (80:20)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.2, shuffle=True, random_state=12345)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVpGJ3aSgz63",
        "outputId": "463d6119-6479-497b-8bf0-ecce7aabfc18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56000, 784)\n",
            "(56000,)\n",
            "(14000, 784)\n",
            "(14000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# le immagini Fashion-MNIST sono immagini in scala di grigio a 8 bit, ogni pixel assume valore [0,255], quindi per\n",
        "# normalizzare in [0,1] basta solo divider tutte le immagini per 255\n",
        "\n",
        "X_train_norm = X_train / 255.\n",
        "X_test_norm  = X_test / 255."
      ],
      "metadata": {
        "id": "QSTJCiwrg-bh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# per questa esercitazione è consigliabile usare una GPU\n",
        "# controlliamo se la GPU è disponibile e nel caso quale tipo di GPU\n",
        "if torch.cuda.is_available():\n",
        "  print('Numero di GPU disponibili: ',torch.cuda.device_count())\n",
        "  for i in range(0,torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(i))\n",
        "\n",
        "# se la GPU è disponibile setto device='cuda', altrimenti 'cpu\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")"
      ],
      "metadata": {
        "id": "JV4zTLvwSEAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb8ac31-d0b4-47d0-b9e7-95ea8913c594"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero di GPU disponibili:  1\n",
            "Tesla T4\n",
            "Computation device: cuda\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conversione in tensori torch dei vettori numpy\n",
        "\n",
        "X_train_pt = torch.Tensor(X_train_norm).float()\n",
        "Y_train_pt = torch.Tensor(Y_train).int()\n",
        "X_test_pt = torch.Tensor(X_test_norm).float()\n",
        "Y_test_pt = torch.Tensor(Y_test).int()\n",
        "\n",
        "# reshape the tensori da (n,784) a (n,1,28,28) come richiesto da conv layer in pytorch\n",
        "\n",
        "X_train_pt = X_train_pt.view((X_train_pt.shape[0],1,28,28))\n",
        "X_test_pt = X_test_pt.view((X_test_pt.shape[0],1,28,28))\n",
        "\n",
        "print(X_train_pt.shape)\n",
        "print(Y_train_pt.shape)\n",
        "print(X_test_pt.shape)\n",
        "print(Y_test_pt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdqZJrGwhJrX",
        "outputId": "832929e4-0362-417e-dfca-17c975c90ce1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([56000, 1, 28, 28])\n",
            "torch.Size([56000])\n",
            "torch.Size([14000, 1, 28, 28])\n",
            "torch.Size([14000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset e dataloaders\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "dataset_train = TensorDataset(X_train_pt, Y_train_pt)\n",
        "train_dl = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True)\n",
        "\n",
        "dataset_test = TensorDataset(X_test_pt, Y_test_pt)\n",
        "test_dl = DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=1, drop_last=True)"
      ],
      "metadata": {
        "id": "f-PryoJkSWZX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definzione del modello di rete neurale.\n",
        "\n",
        "Architettura: VAE convoluzionale con encoder costituito da 2 layer convoluzionali 2D con kernel_size 4 e stride 2 (per comprimere le rappresentazioni), decoder costituito da ConvTranspose2D. Lo spazio latente z è costituito da un layer denso. Usiamo attivazioni ReLu nella parte convoluzionale e sigmoide nell'output in modo da allenare il modello con BCE come reconstruction loss + KL-divergence come regolarizzatore"
      ],
      "metadata": {
        "id": "3Z42_INRiiMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Encoder Model\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, latent_dim=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=hidden_channels,\n",
        "                               kernel_size=4,\n",
        "                               stride=2,\n",
        "                               padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=hidden_channels,\n",
        "                               out_channels=hidden_channels*2,\n",
        "                               kernel_size=4,\n",
        "                               stride=2,\n",
        "                               padding=1)\n",
        "        self.fc_mu = nn.Linear(in_features=hidden_channels*2*7*7,\n",
        "                               out_features=latent_dim)\n",
        "\n",
        "        self.fc_logvar = nn.Linear(in_features=hidden_channels*2*7*7,\n",
        "                               out_features=latent_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "       ######\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.activation(self.conv1(x))\n",
        "        x = self.activation(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "\n",
        "        return x_mu, x_logvar"
      ],
      "metadata": {
        "id": "7o5rmJCFTD_q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Decoder Model\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, latent_dim=2):\n",
        "        super().__init__()\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        self.fc = nn.Linear(in_features=latent_dim,\n",
        "                            out_features=hidden_channels*2*7*7)\n",
        "\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=hidden_channels*2,\n",
        "                                        out_channels=hidden_channels,\n",
        "                                        kernel_size=4,\n",
        "                                        stride=2,\n",
        "                                        padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=hidden_channels,\n",
        "                                        out_channels=1,\n",
        "                                        kernel_size=4,\n",
        "                                        stride=2,\n",
        "                                        padding=1)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), self.hidden_channels*2, 7, 7)\n",
        "        x = self.activation(self.conv2(x))\n",
        "        out = torch.sigmoid(self.conv1(x)) # final activation sigmoid (works with BCELoss as reconstruction loss)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ts3oU_JQUpcy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full VAE/AE Model\n",
        "\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self, encoder,decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_mu, x_logvar = self.encoder(x)\n",
        "    z = self.rep_trick(x_mu, x_logvar)\n",
        "    out = self.decoder(z)\n",
        "    return out, x_mu, x_logvar\n",
        "  def rep_trick(self, x_mu, x_logvar):\n",
        "    if self.training:\n",
        "      # da log var passo a sigma\n",
        "      sigma = (0.5*x_logvar ).exp()# così sono sicuro che sia positivo\n",
        "      # campiono z da G(mu,sigma)\n",
        "      z = x_mu + sigma*torch.randn_like(sigma)# numero dist. normale con stesso tipo e device di sigma\n",
        "      #restituisco z\n",
        "      return z\n",
        "    else:\n",
        "      return x_mu\n",
        ""
      ],
      "metadata": {
        "id": "sKqTgznXVKiV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_channels = 64\n",
        "latent_dims = 2 # è facile da plottare, mettete più alto e poi usate t-sne"
      ],
      "metadata": {
        "id": "g-QNn8A06Gha"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss(recon_x, x,  x_mu, log_var,beta):# output di decoder, GT, media, log  var\n",
        "  recon_loss = F.binary_cross_entropy(recon_x.view(-1,x.size(2)*x.size(3)), x.view(-1,x.size(2)*x.size(3)), reduction='sum')\n",
        "  kl_divergence = -0.5 * torch.sum(1 + log_var - x_mu.pow(2) - log_var.exp())\n",
        "  return recon_loss + beta*kl_divergence, recon_loss, kl_divergence\n",
        ""
      ],
      "metadata": {
        "id": "53-3LAoB61IM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(hidden_channels=hidden_channels, latent_dim=latent_dims)\n",
        "decoder = Decoder(hidden_channels=hidden_channels, latent_dim=latent_dims)\n",
        "model_vae = VAE(encoder,decoder)\n",
        "\n"
      ],
      "metadata": {
        "id": "kyqLK7YS6qUN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "print(encoder(torch.rand(1,1,28,28)))\n",
        "print(decoder(torch.rand(1,latent_dims)).shape)\n",
        "print(model_vae(torch.rand(1,1,28,28))[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBykpGjA6y5g",
        "outputId": "5df79127-e636-40aa-8828-002d21662082"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-0.0069,  0.0203]], grad_fn=<AddmmBackward0>), tensor([[0.0060, 0.0799]], grad_fn=<AddmmBackward0>))\n",
            "torch.Size([1, 1, 28, 28])\n",
            "torch.Size([1, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae_loss(torch.rand(1,1,28,28),torch.rand(1,1,28,28),torch.rand(1,latent_dims),torch.rand(1,latent_dims),1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IXgcTV38wz8",
        "outputId": "8887c866-de32-458d-fa33-ba9f892ec8de"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(809.8184), tensor(809.2694), tensor(0.5490))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xq2uavi69iLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}